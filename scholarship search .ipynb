{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c1eca4f-a531-4741-ab82-9c3cda567198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|                                                                         | 0/10 [00:00<?, ?page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping content from: https://www.scholars4dev.com/category/country/usa-scholarships/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  10%|██████▌                                                          | 1/10 [00:06<00:55,  6.17s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping content from: https://scholarshiproar.com/usa-scholarships/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  20%|█████████████                                                    | 2/10 [00:09<00:38,  4.78s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping content from: https://www.scholars4dev.com/category/scholarships-list/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  30%|███████████████████▌                                             | 3/10 [00:12<00:26,  3.80s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping content from: https://www.belmont.edu/admissions/international/tuition-aid.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  40%|██████████████████████████                                       | 4/10 [00:15<00:19,  3.25s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping content from: https://newyork.thaiembassy.org/en/content/the-list-of-thailand-s-scholarships\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  50%|████████████████████████████████▌                                | 5/10 [00:17<00:15,  3.12s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping content from: https://www.pratt.edu/admissions/undergraduate-admissions/finance-your-education/financial-aid-options/scholarships/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  60%|███████████████████████████████████████                          | 6/10 [00:21<00:13,  3.28s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping content from: http://www.ou.edu/admissions/affordability/scholarships.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  70%|█████████████████████████████████████████████▌                   | 7/10 [00:24<00:09,  3.26s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping content from: https://www.goabroad.com/articles/scholarships-abroad/scholarships-for-study-abroad-around-the-world\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  80%|████████████████████████████████████████████████████             | 8/10 [00:27<00:06,  3.02s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping content from: https://scholarships360.org/scholarships/best-scholarships-for-international-students/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  90%|██████████████████████████████████████████████████████████▌      | 9/10 [00:29<00:02,  2.65s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping content from: https://admissions.psu.edu/costs-aid/scholarships/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|████████████████████████████████████████████████████████████████| 10/10 [00:32<00:00,  3.27s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to scholarship_data_2.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests # a lib used to make http requests  \n",
    "import json\n",
    "import re #regular expresions\n",
    "from bs4 import BeautifulSoup #a lib for extracting information and reading html structures in web pages \n",
    "from tqdm import tqdm #a lib to visulalize progress\n",
    "\n",
    "API_KEY = open('API_KEY.txt').read().strip() #read and save the api key written inside the .txt file\n",
    "SEARCH_ENGINE_ID = open('SEARCH_ENGINE_ID.txt').read().strip() #read and save the search engine id written inside the .txt file\n",
    "\n",
    "search_query = 'list of 2024 scholarships for international students' #this the questions that we will send to the api to make the search quary and obtain results\n",
    "\n",
    "url = 'https://www.googleapis.com/customsearch/v1'#this is the host \"destination\" url\n",
    "\n",
    "params = {   #these are the parameters sent to the api that contains needed information to gain access and other things for search inhancements\n",
    "    'q' : search_query,\n",
    "    'key' : API_KEY,\n",
    "    'cx' : SEARCH_ENGINE_ID,\n",
    "    #'dateRestrict': '2024' is to get the search results that only fits with a certain publish day lets say 2024\n",
    "}\n",
    "\n",
    "response = requests.get(url,params=params) # send a get request to the url identified with the given params \n",
    "results = response.json()['items'] #extracts the list or dictionary that is associated with the key items from the json responce \n",
    "\n",
    "keywords = [ #identifies keywords for href filtering \n",
    "    \"Scholarship\", \"Scholarships\", \"Grant\", \"Fellowship\", \"Financial aid\", \"Study abroad\",\n",
    "    \"International students\", \"Education funding\", \"Tuition assistance\", \"Student grants\",\n",
    "    \"Educational opportunities\", \"Academic funding\", \"Student scholarships\", \"College funding\",\n",
    "    \"Higher education support\", \"Award\", \"Bursary\", \"Sponsorship\", \"Endowment\", \"Merit-based\",\n",
    "    \"Need-based\", \"Financial assistance\", \"Student aid\", \"Educational grants\", \"Study grants\",\n",
    "    \"Undergraduate\", \"Graduate\", \"Postgraduate\", \"PhD\", \"Master's\", \"Bachelor's\", \"Doctoral\",\n",
    "    \"Academic support\", \"Educational funding\", \"Student funding\", \"College scholarships\",\n",
    "    \"University scholarships\", \"Minority scholarships\", \"Diversity scholarships\", \"Research funding\"\n",
    "]\n",
    "\n",
    "# Regular expression pattern for matching http or https URLs\n",
    "url_pattern = re.compile(r'^https?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+$')\n",
    "\n",
    "def fetch_page_content(link): #function for error handling \n",
    "    response = requests.get(link)\n",
    "    if response.status_code == 200: #if the status code of the responce is 200 then the link is valid and we can get the text out of it \n",
    "        return response.text\n",
    "    else: # the error handling part if its not 200 then failed to fetch content from the link provided\n",
    "        print(f\"Failed to fetch content from {link}\")\n",
    "        return None\n",
    "\n",
    "#by using the nested functions approach for organizing code and clarity\n",
    "def extract_text(html): #function that handles text extraction from the html structure of a provided link\n",
    "    soup = BeautifulSoup(html, 'html.parser') #creates a beautifulSoup object by parsing the provided html content with the spacified parser\n",
    "    \n",
    "    # Remove script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "\n",
    "    # Define classes to remove\n",
    "    classes_to_remove = ['script', 'javascript', 'code', 'ad', 'advertisement', 'ad-banner', 'ad-block', 'menu', 'nav', 'navbar', 'navigation', 'footer']\n",
    "    for class_name in classes_to_remove:\n",
    "        for element in soup.find_all(class_=class_name):\n",
    "            element.extract()\n",
    "\n",
    "    def extract_content(tag):# this is the nested function that takes a tag as a parameter\n",
    "        if tag.name == 'a' and 'href' in tag.attrs:\n",
    "            return f\"{tag.get_text(separator='\\n', strip=True)} (link: {tag['href']})\" #return the text of the tag follow by the link in parantheses\n",
    "        else:\n",
    "            return tag.get_text(separator='\\n', strip=True) #return the text of the tag\n",
    "\n",
    "    parts = [] #empty list to store parts of the extracted text\n",
    "    for tag in soup.find_all(True):  # True finds all tags\n",
    "        if tag.name == 'a' and 'href' in tag.attrs:\n",
    "            parts.append(extract_content(tag)) #use the extract_content function to process and add it to parts.\n",
    "        elif tag.name != 'a':\n",
    "            # Handle nested <a> tags within other tags\n",
    "            nested_links = tag.find_all('a', href=True)\n",
    "            if nested_links: #if there are nested anchor tags with 'href' attributes process each nested tag and add it to parts \n",
    "                for nested_tag in nested_links:\n",
    "                    parts.append(extract_content(nested_tag))\n",
    "            else: #no nested anchor tags, process and add it to parts\n",
    "                parts.append(tag.get_text(separator='\\n', strip=True))\n",
    "\n",
    "    text = '\\n'.join(part for part in parts if part.strip()) #join all parts with newline characters\n",
    "    text = re.sub(r'\\\\n', '\\n', text) #replace escaped newline characters with actual newline characters \n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_href(html):\n",
    "    soup = BeautifulSoup(html , 'html.parser')\n",
    "    hrefs = [a.get('href') for a in soup.find_all('a') if a.get('href') and url_pattern.match(a.get('href')) ] #find all hrefs in a html web page structure \n",
    "    relevant_hrefs =  []\n",
    "    for href in hrefs:\n",
    "        if any(keyword.lower() in href.lower() for keyword in keywords): #if it contains any of the of the keywords add it to the relevent href list\n",
    "            relevant_hrefs.append(href) \n",
    "    return relevant_hrefs\n",
    "\n",
    "\n",
    "#used functional decomposition to break down the crawler algorithm into smaller functions that are easier to understand, manage, and maintain\n",
    "def crawler(url , depth =1 , max_depth = 10): \n",
    "    if depth > max_depth: #for the number of links access at one time \"stop case\"\n",
    "        return \"exceeded the depth too deep\"\n",
    "\n",
    "    html_content = fetch_page_content(url) #check for the usability of the link \n",
    "    if html_content:\n",
    "        content = extract_text(html_content)\n",
    "        hrefs = extract_href(html_content)\n",
    "        output_data = { #create the dic for the shape of the output i want after scraping the content from the web page \n",
    "            'url': url,\n",
    "            'content': content,\n",
    "            'hrefs': hrefs\n",
    "        }\n",
    "        return output_data\n",
    "    #try:     //commented for efficient time using if uncomment if will take a lot of time in processing\n",
    "       #for href in hrefs:\n",
    "         #crawler(href, depth + 1, max_depth)\n",
    "    #except Exception as e:\n",
    "            #print(f\"Error crawling {url}: {e}\")\n",
    "\n",
    "output_file = 'scholarship_data_2.json' #destination file \n",
    "with open(output_file, 'w') as f:\n",
    "    for item in tqdm(results, desc=\"Scraping pages\", unit=\"page\"):\n",
    "        link = item['link']\n",
    "        print(f\"Scraping content from: {link}\")\n",
    "        json.dump(crawler(link), f) #save the data in json format for each page \"dump them in one json file\"\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
